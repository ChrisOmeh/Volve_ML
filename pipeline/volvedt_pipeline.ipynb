{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install --user --upgrade pip\n",
    "\n",
    "!pip3 install lasio pandas==0.24.2 matplotlib==3.2.2 scipy==1.4.1 statsmodels==0.12.0 scikit-learn==0.23.1 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if the install was successful\n",
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Kubeflow SDK\n",
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where the outputs are stored\n",
    "out_dir = \"/home/jovyan/Volve_ML/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tranform(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'ipython'])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    \n",
    "    # Download the dataset and split into training and test data. \n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/Josepholaidepetro/Volve_ML/main/data/train.csv\")\n",
    "    \n",
    "    # Preprocess\n",
    "    data.drop(['DEPTH', 'BS', 'RD', 'ROP', 'RM', 'DRHO'], axis=1, inplace=True)\n",
    "    \n",
    "    # If Nan, drop\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # transform the RT to logarithmic\n",
    "    data['RT'] = np.log10(data['RT'])\n",
    "    \n",
    "    # perform a yeo-johnson transform of the train dataset\n",
    "    ptrain = PowerTransformer(method='yeo-johnson')\n",
    "    train_df_yj = ptrain.fit_transform(data.drop('DT', axis=1))\n",
    "    train_df_yj_norm = pd.DataFrame(train_df_yj, columns=data.columns.drop('DT'))\n",
    "    y_train = data['DT']\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/train_data', 'wb') as f:\n",
    "        pickle.dump((train_df_yj_norm,  y_train), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tranform(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tranform(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'ipython'])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import PowerTransformer\n",
    "    \n",
    "    # Download the dataset and split into training and test data. \n",
    "    data = pd.read_csv(\"https://raw.githubusercontent.com/Josepholaidepetro/Volve_ML/main/data/test.csv\")\n",
    "    \n",
    "    # Preprocess\n",
    "    data.drop(['DEPTH', 'BS', 'ROP', 'DRHO'], axis=1, inplace=True)\n",
    "    \n",
    "    # If Nan, drop\n",
    "    data.dropna(inplace=True)\n",
    "    \n",
    "    # transform the RT to logarithmic\n",
    "    data['RT'] = np.log10(data['RT'])\n",
    "    \n",
    "    # perform a yeo-johnson transform of the train dataset\n",
    "    ptest = PowerTransformer(method='yeo-johnson')\n",
    "    test_df_yj = ptest.fit_transform(data.drop('DT', axis=1))\n",
    "    test_df_yj_norm = pd.DataFrame(test_df_yj, columns=data.columns.drop('DT'))\n",
    "    y_test = data['DT']\n",
    "    \n",
    "    #Save the test_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/test_data', 'wb') as f:\n",
    "        pickle.dump((test_df_yj_norm,  y_test), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tranform(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Outlier_removal(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'ipython'])\n",
    "    \n",
    "    from sklearn.svm import OneClassSVM\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load and unpack the train_data\n",
    "    with open(f'{data_path}/train_data','rb') as f:\n",
    "        train_data = pickle.load(f)\n",
    "    # Separate the train_df_yj_norm from y_train.\n",
    "    train_df_yj_norm,  y_train = train_data\n",
    "\n",
    "    # Method 4: One-class SVM\n",
    "    svm = OneClassSVM(nu=0.13)\n",
    "    yhat = svm.fit_predict(train_df_yj_norm)\n",
    "    mask = yhat != -1\n",
    "    train_df_svm = train_df_yj_norm[mask]\n",
    "    y_train_svm = y_train[mask]\n",
    "\n",
    "    # prepare train data for modelling\n",
    "    X_train = train_df_svm.drop('label', axis=1)\n",
    "    y_train = y_train_svm.copy()\n",
    "    \n",
    "    #Save the train_data as a pickle file to be used by the predict component.\n",
    "    with open(f'{data_path}/train_data2', 'wb') as f:\n",
    "        pickle.dump((X_train,  y_train), f)\n",
    "    \n",
    "    print('Outlier removed successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Outlier_removal(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cython\n",
    "print(cython.__version__)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_building(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'cython==0.27.3'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import cython\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    "    # Load and unpack the train_data\n",
    "    with open(f'{data_path}/train_data2','rb') as f:\n",
    "        train_data2 = pickle.load(f)\n",
    "    # Separate the train_df_yj_norm from y_train.\n",
    "    X_train,  y_train = train_data2\n",
    "    \n",
    "    # Load and unpack the test_data\n",
    "    with open(f'{data_path}/test_data','rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the train_df_yj_norm from y_train.\n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    # Define the model.\n",
    "    model = ExtraTreesRegressor(n_estimators=800, max_depth=6, random_state=21)\n",
    "\n",
    "    # Run a training job\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    #Evaluate the model and print the results\n",
    "    score = model.score(X_test, y_test)\n",
    "    print(\"R-squared of Well 3: {}\".format(score))\n",
    "\n",
    "    \n",
    "    #Save the model to the designated \n",
    "    filename = f'{data_path}/modelo.pkl'\n",
    "    pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = model_building(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_path):\n",
    "    \n",
    "    import pickle\n",
    "    import sys, subprocess;\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'pandas==0.24.2'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'scikit-learn==0.23.1'])\n",
    "    subprocess.run([sys.executable, '-m', 'pip', 'install', 'cython==0.27.3'])\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import cython\n",
    "    from sklearn.ensemble import ExtraTreesRegressor\n",
    "    \n",
    "    # Load the saved Keras model\n",
    "    filename = f'{data_path}/modelo.pkl'\n",
    "    regressor = pickle.load(open(filename, 'rb'))\n",
    "\n",
    "    # Load and unpack the test_data\n",
    "    with open(f'{data_path}/test_data','rb') as f:\n",
    "        test_data = pickle.load(f)\n",
    "    # Separate the X_test from y_test.\n",
    "    test_df_yj_norm,  y_test = test_data\n",
    "\n",
    "    # make predictions.\n",
    "    y_pred = regressor.predict(test_df_yj_norm)\n",
    "\n",
    "    \n",
    "    with open(f'{data_path}/result1', 'wb') as f:\n",
    "        pickle.dump(y_pred, f)\n",
    "        \n",
    "    with open(f'{data_path}/result.txt', 'w') as result:\n",
    "        result.write(\" Prediction: {}, Actual: {} \".format(y_pred,y_test))\n",
    "    \n",
    "    print('Prediction has be saved successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(out_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and predict lightweight components.\n",
    "train_tranform_op = comp.func_to_container_op(train_tranform , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "test_tranform_op = comp.func_to_container_op(test_tranform , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "Outlier_removal_op = comp.func_to_container_op(Outlier_removal , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "train_op = comp.func_to_container_op(model_building , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "predict_op = comp.func_to_container_op(predict , base_image = \"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Kubeflow Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a client to enable communication with the Pipelines API server.\n",
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pipeline\n",
    "@dsl.pipeline(\n",
    "   name='Sonic Log Prediction Pipeline',\n",
    "   description='An ML pipeline that performs Sonic Log Prediction prediction.'\n",
    ")\n",
    "\n",
    "# Define parameters to be fed into pipeline\n",
    "def dt_container_pipeline(\n",
    "    data_path: str\n",
    "):\n",
    "    \n",
    "    # Define volume to share data between components.\n",
    "    vop = dsl.VolumeOp(\n",
    "    name=\"create_volume\",\n",
    "    resource_name=\"data-volume\", \n",
    "    size=\"1Gi\", \n",
    "    modes=dsl.VOLUME_MODE_RWO)\n",
    "    \n",
    "    # Create deploy component.\n",
    "    train_tranform_container = train_tranform_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: vop.volume})\n",
    "\n",
    "    # Create data transformation component.\n",
    "    test_tranform_container = test_tranform_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: train_tranform_container.pvolume})\n",
    "    \n",
    "    # Create model validation component.\n",
    "    Outlier_removal_container = Outlier_removal_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: test_tranform_container.pvolume})\n",
    "    \n",
    "    # Create model validation component.\n",
    "    train_container = train_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: Outlier_removal_container.pvolume})\n",
    "    \n",
    "    # Create model training component.\n",
    "    predict_container = predict_op(data_path) \\\n",
    "                                    .add_pvolumes({data_path: train_container.pvolume})\n",
    "    \n",
    "    \n",
    "    # Print the result of the prediction\n",
    "    churn_result_container = dsl.ContainerOp(\n",
    "        name=\"print_prediction\",\n",
    "        image='library/bash:4.4.23',\n",
    "        pvolumes={data_path: predict_container.pvolume},\n",
    "        arguments=['cat', f'{data_path}/result.txt']\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = dt_container_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = out_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH='modelo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'dt_prediction_kubeflow'\n",
    "run_name = pipeline_func.__name__ + ' run'\n",
    "\n",
    "arguments = {\"data_path\":DATA_PATH}\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, '{}.zip'.format(experiment_name))\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://localhost:8888/pipeline#/experiments/details/644a33c5-3114-4b3c-b6de-3b0b6467eafa\n",
    "\n",
    "http://localhost:8888/pipeline#/runs/details/2aadb518-ead4-4035-90ea-011a44bfe16d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
